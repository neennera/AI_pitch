{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 960x640 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa as lib\n",
    "import torch\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "from scipy.io import wavfile\n",
    "from torchaudio.compliance import kaldi\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "figure(figsize=(12, 8), dpi=80)\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "from scipy.io import wavfile\n",
    "from torchaudio.compliance import kaldi\n",
    "from tqdm.auto import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MUSICNET_DIR = \"D:\\\\AI_MIDI\\\\midisynth_dataset-v1\\\\wav\"\n",
    "wav_paths = sorted(glob(f\"{MUSICNET_DIR}/*.wav\", recursive=True))\n",
    "save_path = \"D:\\\\AI_MIDI\\\\processed\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#สร้าง classmap ระหว่างโน๊ตและ index เพื่อให้ง่ายต่อโมเดล\n",
    "df = pd.read_csv(\"D:\\\\AI_MIDI\\\\midisynth_dataset-v1\\\\labels.csv\")\n",
    "classm = df[\"note\"].unique().tolist()\n",
    "classmap=[]\n",
    "for i in range(0,10):\n",
    "    for c in 'CDEFGAB':\n",
    "        if (c+str(i) in classm) :\n",
    "            classmap.append(c+str(i))\n",
    "        if (c+'#'+str(i) in classm) :\n",
    "            classmap.append(c+'#'+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test assign\n",
    "df['train'] = np.zeros(26643) == 0\n",
    "for note in df[\"note\"].unique():\n",
    "    index = df.index[df['note'] == note].tolist()\n",
    "    choices = random.sample(index, k= int(0.2*len(index)))\n",
    "    for id in choices:\n",
    "        df['train'][id]=False\n",
    "\n",
    "count = np.zeros((2,107), dtype=\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DATA_Prepare():\n",
    "    def __init__(self, transformation, device):\n",
    "        self.MUSICNET_DIR = \"D:\\\\AI_MIDI\\\\midisynth_dataset-v1\\\\wav\"\n",
    "        self.target_sample_rate = 16000.0\n",
    "        self.device = device\n",
    "        self.transformation = transformation.to(self.device)\n",
    "        self.wav_paths = sorted(glob(f\"{self.MUSICNET_DIR}/*.wav\", recursive=True)) \n",
    "        self.count_path = len(wav_paths)\n",
    "\n",
    "    def get_data(self,st,ed) :\n",
    "        for i in range(st,ed):\n",
    "            if i == self.count_path :\n",
    "                return i\n",
    "            wav_path = self.wav_paths[i]\n",
    "            signal, sr = torchaudio.load(wav_path)\n",
    "            signal = self._resample_if_necessary(signal, sr)\n",
    "            signal = self._mix_down_if_necessary(signal)\n",
    "            signal = self.transformation(signal)\n",
    "            if (wav_path[38]!='_') :\n",
    "                label = classmap.index(wav_path[36:39])\n",
    "            else :\n",
    "                label = classmap.index(wav_path[36:38])\n",
    "                    \n",
    "            self.save_data(signal,label,i)\n",
    "            with open(\"dump_i.txt\", \"wb\") as f:\n",
    "                pickle.dump(i+1,f)\n",
    "            with open(\"dump_count.npy\", \"wb\") as f:\n",
    "                pickle.dump(count,f) \n",
    "        return ed+1\n",
    "\n",
    "    def save_data(self,signal,label,i):        \n",
    "        datatype = \"train\" if df[\"train\"][i] else \"test\"\n",
    "        #label-count\n",
    "        ty = 1*(datatype == 'train')\n",
    "        if len(str(count[ty][label])) == 1 :\n",
    "            ct = '0'+str(count[ty][label])\n",
    "        else :\n",
    "            ct = str(count[ty][label])\n",
    "        count[ty][label]+=1\n",
    "\n",
    "        #label\n",
    "        if len(str(label))==1:\n",
    "            label = '0'+str(label)\n",
    "        else :\n",
    "            label = str(label)\n",
    "\n",
    "        with open(f\"{save_path}/{datatype}/{label}-{ct}.npy\", \"wb\") as f:\n",
    "            np.save(f, signal.numpy())\n",
    "    \n",
    "    def _resample_if_necessary(self, signal, sr):\n",
    "        if sr != self.target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n",
    "            signal = resampler(signal)\n",
    "        return signal\n",
    "\n",
    "    def _mix_down_if_necessary(self, signal):\n",
    "        if signal.shape[0] > 1:\n",
    "            signal = torch.mean(signal, dim=0, keepdim=True)\n",
    "        return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"dump_count.npy\", 'rb') as f:\n",
    "    count = pickle.load(f)\n",
    "with open(\"dump_i.txt\", 'rb') as f:\n",
    "    i = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done 26644 in 26643\n"
     ]
    }
   ],
   "source": [
    " mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate=34624,\n",
    "    n_mels=254,\n",
    "    n_fft=2048,\n",
    "    hop_length=512\n",
    ")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "feat = DATA_Prepare(mel_spectrogram, device)\n",
    "i = feat.get_data(i,len(wav_paths))\n",
    "print(f\"done {i} in {len(wav_paths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "id = 64\n",
    "print(classmap[data[id][\"label\"]])\n",
    "sns.heatmap(data[id][\"data\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "from scipy.io import wavfile\n",
    "from torchaudio.compliance import kaldi\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def sample_to_frames(\n",
    "    t_sample,\n",
    "    frame_size=10, # ms / frame\n",
    "    sampling_rate=44100\n",
    "):\n",
    "    t_sec = t_sample / sampling_rate  # (sample) / (sample / sec)\n",
    "    t_frame = t_sec / (frame_size / 1000) # (sec) / (sec / 1000*frame)\n",
    "    return int(t_frame)\n",
    "\n",
    "\n",
    "def get_data(set_name=\"train\", n_notes=88):\n",
    "    # check if set_name is valid or not\n",
    "    if set_name.lower().strip() not in [\"train\", \"test\"]:\n",
    "        raise NameError(f\"Unrecognized set name: {set_name}\")\n",
    "    \n",
    "    wav_paths = sorted(glob(f\"{MUSICNET_DIR}/{set_name}_data/**/*.wav\", recursive=True))\n",
    "    csv_paths = [wav.replace(\"_data\", \"_labels\").replace(\".wav\", \".csv\") for wav in wav_paths]\n",
    "    #print(csv_paths)\n",
    "    assert all(os.path.exists(csv) for csv in csv_paths)\n",
    "    \n",
    "    data = []\n",
    "    for wav_path, csv_path in tqdm(zip(wav_paths, csv_paths), total=len(wav_paths)):\n",
    "        wav, sr = torchaudio.load(wav_path)\n",
    "        # config feature here\n",
    "        fbank = kaldi.fbank(wav, sample_frequency=sr, num_mel_bins=120, frame_length=30)\n",
    "        \n",
    "        label = pd.read_csv(csv_path)\n",
    "        y = np.zeros([fbank.shape[0], n_notes])\n",
    "        for  i, row in label.iterrows():\n",
    "            y[sample_to_frames(row[\"start_time\"]):sample_to_frames(row[\"end_time\"]), row[\"note\"] - 20] = 1\n",
    "            \n",
    "        data.append({\"wav_path\": wav_path, \"label\": y, \"feature\": fbank})\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UrbanSoundDataset(Dataset):\n",
    "\n",
    "    def __init__(self,\n",
    "                 annotations_file,\n",
    "                 audio_dir,\n",
    "                 transformation,\n",
    "                 target_sample_rate,\n",
    "                 num_samples,\n",
    "                 device):\n",
    "        self.annotations = pd.read_csv(annotations_file)\n",
    "        self.audio_dir = audio_dir\n",
    "        self.device = device\n",
    "        self.transformation = transformation.to(self.device)\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        audio_sample_path = self._get_audio_sample_path(index)\n",
    "        label = self._get_audio_sample_label(index)\n",
    "        signal, sr = torchaudio.load(audio_sample_path)\n",
    "        signal = signal.to(self.device)\n",
    "        signal = self._resample_if_necessary(signal, sr)\n",
    "        signal = self._mix_down_if_necessary(signal)\n",
    "        signal = self._cut_if_necessary(signal)\n",
    "        signal = self._right_pad_if_necessary(signal)\n",
    "        signal = self.transformation(signal)\n",
    "        return signal, label\n",
    "\n",
    "    def _cut_if_necessary(self, signal):\n",
    "        if signal.shape[1] > self.num_samples:\n",
    "            signal = signal[:, :self.num_samples]\n",
    "        return signal\n",
    "\n",
    "    def _right_pad_if_necessary(self, signal):\n",
    "        length_signal = signal.shape[1]\n",
    "        if length_signal < self.num_samples:\n",
    "            num_missing_samples = self.num_samples - length_signal\n",
    "            last_dim_padding = (0, num_missing_samples)\n",
    "            signal = torch.nn.functional.pad(signal, last_dim_padding)\n",
    "        return signal\n",
    "\n",
    "    def _resample_if_necessary(self, signal, sr):\n",
    "        if sr != self.target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate)\n",
    "            signal = resampler(signal)\n",
    "        return signal\n",
    "\n",
    "    def _mix_down_if_necessary(self, signal):\n",
    "        if signal.shape[0] > 1:\n",
    "            signal = torch.mean(signal, dim=0, keepdim=True)\n",
    "        return signal\n",
    "\n",
    "    def _get_audio_sample_path(self, index):\n",
    "        fold = f\"fold{self.annotations.iloc[index, 5]}\"\n",
    "        path = os.path.join(self.audio_dir, fold, self.annotations.iloc[\n",
    "            index, 0])\n",
    "        return path\n",
    "\n",
    "    def _get_audio_sample_label(self, index):\n",
    "        return self.annotations.iloc[index, 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATALOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cpu\n",
      "There are 8732 samples in the dataset.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    ANNOTATIONS_FILE = \"D:\\\\AI_pitch_Data\\\\UrbanSound8K\\\\metadata\\\\UrbanSound8K.csv\"\n",
    "    AUDIO_DIR = \"D:\\\\AI_pitch_Data\\\\UrbanSound8K\\\\audio\"\n",
    "    SAMPLE_RATE = 22050\n",
    "    NUM_SAMPLES = 22050\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "    print(f\"Using device {device}\")\n",
    "\n",
    "    mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
    "        sample_rate=SAMPLE_RATE,\n",
    "        n_fft=1024,\n",
    "        hop_length=512,\n",
    "        n_mels=64\n",
    "    )\n",
    "\n",
    "    usd = UrbanSoundDataset(ANNOTATIONS_FILE,\n",
    "                            AUDIO_DIR,\n",
    "                            mel_spectrogram,\n",
    "                            SAMPLE_RATE,\n",
    "                            NUM_SAMPLES,\n",
    "                            device)\n",
    "    print(f\"There are {len(usd)} samples in the dataset.\")\n",
    "    signal, label = usd[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_dataloader = DataLoader(usd, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class CNNNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 4 conv blocks / flatten / linear / softmax\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1,\n",
    "                out_channels=16,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=16,\n",
    "                out_channels=32,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=32,\n",
    "                out_channels=64,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=64,\n",
    "                out_channels=128,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(128 * 5 * 4, 10)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        x = self.conv1(input_data)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear(x)\n",
    "        predictions = self.softmax(logits)\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 10\n",
    "lr = 0.001\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model=CNNNetwork().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, data_loader, loss_fn, optimiser, device):\n",
    "  for inputs, targets in data_loader :\n",
    "    inputs, targets = inputs.to(device), targets.to(device)\n",
    "  \n",
    "  #calculate loss\n",
    "  pred = model(inputs)\n",
    "  loss = loss_fn(pred, targets)\n",
    "  #acc = Acc_fn(pred, targets)\n",
    "\n",
    "  #backpropagate + update weight\n",
    "  optimiser.zero_grad() #ลบ gradiun จาก batch ก่อนๆ\n",
    "  loss.backward()\n",
    "  optimiser.step()\n",
    "\n",
    "  print(f\"Loss : {loss.item()}\") #\\t Acc:{acc}\")\n",
    "\n",
    "def train(model, data_loader, loss_fn, optimiser, device, epoch):\n",
    "  for i in range(epoch):\n",
    "    print(f\"Epoch : {i+1}\")\n",
    "    train_one_epoch(model, data_loader, loss_fn, optimiser, device)\n",
    "    print(\"---------------------------\")\n",
    "  print(\"Finished training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, train_dataloader, loss_fn, optimiser, device, epochs)\n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Trained feed forward net saved at feedforwardnet.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c9bf447bc3f95dcfbc19cb1a84d6b160112105653e59c16eb73ab72854d9f644"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

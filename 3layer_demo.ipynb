{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import pickle\n",
    "import numpy as np                                       # fast vectors and matrices\n",
    "from glob import glob\n",
    "from time import time\n",
    "import tensorflow as tf\n",
    "'''\n",
    "import matplotlib.pyplot as plt                          # plotting\n",
    "from scipy.fftpack import fft\n",
    "from sklearn.metrics import average_precision_score\n",
    "'''\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\WINDOW X\\anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:111: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "sys.path.insert(0,'library\\\\')\n",
    "import config\n",
    "import diagnostics\n",
    "import base_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadData(song):\n",
    "    data_paths = sorted(glob(f\"processed_dataset\\\\feat\\\\{song}-*.npy\", recursive=True))\n",
    "    lebel_paths = sorted(glob(f\"processed_dataset\\\\label\\\\{song}-*.npy\", recursive=True))\n",
    "    data = []\n",
    "    label = []\n",
    "    \n",
    "    for i in range(len(data_paths)) : \n",
    "        data.append(np.load(data_paths[i],\"r\").tolist())\n",
    "        label.append(np.load(lebel_paths[i],\"r\").tolist())\n",
    "    return data,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, label  = LoadData(1727)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## not touch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas, labels = {},{}\n",
    "datas[1727-00] = data\n",
    "labels[1727-00] = label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data : list of list [1500] <br>\n",
    "data[i] = wav after pass the filter bank in scale [0,120]\n",
    "\n",
    "label : list of list [1500] <br>\n",
    "label[i] = 0/1 of if note in index (0-88) is playing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.arange(0,len(label))\n",
    "for i in range(len(label)):\n",
    "    for j in range(88) :\n",
    "        if(label[i][j]==1) :\n",
    "            plt.scatter(i,j, s=2,c=\"b\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.arange(0,len(label))\n",
    "for i in range(len(label)):\n",
    "    #plt.plot([pt[i] for pt in label])\n",
    "    for j in range(88) :\n",
    "        if(label[i][j]==1) :\n",
    "            plt.scatter(i,j, s=2,c=\"b\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Spectrograms(base_model.Model):\n",
    "#class Spectrograms():\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(Spectrograms, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def define_graph(self):\n",
    "        super(Spectrograms, self).define_graph()\n",
    "        \n",
    "        # lvl1 convolutions are shared between regions\n",
    "        self.k = 512              # lvl1 nodes\n",
    "        self.d = 4096              # lvl1 receptive field\n",
    "        \n",
    "        self.k2 = 512              # lvl2 nodes\n",
    "\n",
    "        # number of lvl1 features\n",
    "        regions = 1 + (self.window - self.d)/self.stride\n",
    "        print('Number of V1 feature regions: {}'.format(regions))\n",
    "\n",
    "        wscale = 10e-3\n",
    "        with tf.compat.v1.variable_scope('parameters'):\n",
    "            w = tf.Variable(wscale*tf.compat.v1.random_normal([1,self.d,1,self.k],seed=999))\n",
    "            wavg = self.register_weights(w,'w',average=.9998)\n",
    "            w1 = tf.Variable(wscale*tf.compat.v1.random_normal([(int)(regions*self.k),self.k2],seed=999))\n",
    "            w1avg = self.register_weights(w1,'w1',average=.9998)\n",
    "            beta = tf.Variable(wscale*tf.compat.v1.random_normal([self.k2,self.m],seed=999))\n",
    "            betaavg = self.register_weights(beta,'beta',average=.9998)\n",
    "\n",
    "        with tf.compat.v1.variable_scope('queued_model'):\n",
    "            zx = tf.compat.v1.log(tf.constant(1.) + tf.nn.relu(tf.nn.conv2d(self.xq,w,strides=[1,1,self.stride,1],padding='VALID')))\n",
    "            z2 = tf.nn.relu(tf.matmul(tf.reshape(zx,[int(self.batch_size),(int)(regions*self.k)]),w1))\n",
    "            y = tf.matmul(z2,beta)\n",
    "            self.loss = tf.reduce_mean(tf.nn.l2_loss(y-tf.reshape(self.yq,[self.batch_size,self.m])))\n",
    "\n",
    "        with tf.compat.v1.variable_scope('direct_model'):\n",
    "            zx = tf.compat.v1.log(tf.constant(1.) + tf.nn.relu(tf.nn.conv2d(self.xd,wavg,strides=[1,1,self.stride,1],padding='VALID')))\n",
    "            z2 = tf.nn.relu(tf.matmul(tf.reshape(zx,[tf.shape(self.xd)[0],int(regions*self.k)]),w1avg))\n",
    "            self.y_direct = tf.matmul(z2,betaavg)\n",
    "            self.loss_direct = tf.reduce_mean(tf.nn.l2_loss(self.y_direct-self.yd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame()\n",
    "\n",
    "model = Spectrograms(df,checkpoint_path='convnet_mlp_raw/', outputs=1, window=16384, mmap=True, batch_size=32,\n",
    "                    normalize=True, extended_test_set=True, use_mirex=False, init=False, pitch_transforms=5, jitter=.1,\n",
    "                    restrict=False)\n",
    "'''\n",
    "model = Spectrograms(df,checkpoint_path='convnet_mlp_raw/', outputs=1, window=16384, mmap=True, batch_size=200,\n",
    "                    normalize=True, extended_test_set=True, use_mirex=False, init=False, pitch_transforms=5, jitter=.1,\n",
    "                    restrict=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using dataframe แทน list ที่มี key [อาจจะเป็น dict มั้ง]\n",
    "import pandas as pd\n",
    "#df = pd.DataFrame(list(zip(data, label)), columns =['feat', 'label'])\n",
    "lb={}\n",
    "lb[1727]=label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=os.listdir(config.records_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of V1 feature regions: 25.0\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 24] Too many open files: 'processed_dataset/feat/1742-36.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-1bfd43450515>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# start up the session, kick off the worker threads, restore checkpoint, etc.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\AI_pitch\\library\\base_model.py\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    243\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mrecord\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfeat_path\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmmap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 245\u001b[1;33m                 \u001b[0mfd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecords_path\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mrecord\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mO_RDONLY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m                 \u001b[0mbuff\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmmap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccess\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmmap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACCESS_READ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbuff\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuff\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0msz_float\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 24] Too many open files: 'processed_dataset/feat/1742-36.npy'"
     ]
    }
   ],
   "source": [
    "lr = .001/3/3/3\n",
    "mom = .95\n",
    "\n",
    "# stop the model if it is already running\n",
    "#model.stop()\n",
    "\n",
    "# we have to rebuild the graph every time because input queues can't be reopened\n",
    "model.define_graph()\n",
    "\n",
    "learning_rate = tf.compat.v1.placeholder(tf.float32, shape=[])\n",
    "opt_op = tf.compat.v1.train.MomentumOptimizer(learning_rate,mom).minimize(model.loss)\n",
    "with tf.control_dependencies([opt_op]):\n",
    "    train_step = tf.group(*model.averages)\n",
    "\n",
    "# start up the session, kick off the worker threads, restore checkpoint, etc.\n",
    "model.start()\n",
    "\n",
    "try:\n",
    "    ptime = time()\n",
    "    print(model.status_header())\n",
    "    while True:\n",
    "        if model.iter % 1000 == 0:\n",
    "            model.update_status(ptime,time(),lr)\n",
    "            model.checkpoint()\n",
    "            print(model.status())\n",
    "            ptime = time()\n",
    "\n",
    "        model.sess.run(train_step, feed_dict={learning_rate: lr})\n",
    "        model.iter += 1\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    model.checkpoint()\n",
    "    print('Graceful Exit')\n",
    "finally:\n",
    "    model.stop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c9bf447bc3f95dcfbc19cb1a84d6b160112105653e59c16eb73ab72854d9f644"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n",
    "    img, label = training_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(labels_map[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "feat_path=\"processed_dataset/feat/\"\n",
    "label_path=\"processed_dataset/label/\"\n",
    "\n",
    "record=os.listdir(feat_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_name= [item for item in record if item[0]=='1']\n",
    "test_name= [item for item in record if item[0]=='2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadData() : \n",
    "    for item in record :\n",
    "        data = torch.from_numpy(np.load(feat_path + item))\n",
    "        label = torch.from_numpy(np.load(label_path + item))\n",
    "        print(data, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, annotations_file, transform=None, target_transform=None):\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if status==1 :\n",
    "            item= train_name[idx]\n",
    "        else :\n",
    "            item=test_name[idx]\n",
    "        feat = np.load(feat_path + item)\n",
    "        label = np.load(label_path + item)\n",
    "        if self.transform:\n",
    "            feat = self.transform(feat)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return feat, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-e343d8476403>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtraining_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'training_data' is not defined"
     ]
    }
   ],
   "source": [
    "training_data\n",
    "test_data\n",
    "status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-63-039a7527717d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mCustomDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mstatue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mCustomDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'training_data' is not defined"
     ]
    }
   ],
   "source": [
    "status=1\n",
    "CustomDataset(training_data)\n",
    "statue=2\n",
    "CustomDataset(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATALOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train = DataLoader(training_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'yield' outside function (<ipython-input-46-5f3764ea4fcb>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-46-5f3764ea4fcb>\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    yield collate_fn([next(dataset_iter) for _ in indices])\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m 'yield' outside function\n"
     ]
    }
   ],
   "source": [
    "dataset_iter = iter(dataset)\n",
    "for indices in batch_sampler:\n",
    "    yield collate_fn([next(dataset_iter) for _ in indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CustomDataset(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "938"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([64, 1, 28, 28])\n",
      "Labels batch shape: torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display image and label.\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "img = train_features[0].squeeze()\n",
    "label = train_labels[0]\n",
    "plt.imshow(img, cmap=\"gray\")\n",
    "plt.show()\n",
    "type(label)\n",
    "#print(\"Label: \",labels_map[f\"label\"])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c9bf447bc3f95dcfbc19cb1a84d6b160112105653e59c16eb73ab72854d9f644"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
